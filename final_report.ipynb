{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9ea977-6b67-4b2c-af3c-49c92d23713f",
   "metadata": {},
   "source": [
    "# Predicting the presence of heart disease using a patient’s age, maximum heart rate and ST depression\n",
    "## Introduction\n",
    "Given our preliminary data exploration conducted with the Cleveland Heart Disease data set (more on this below), we initially chose to use 4 variables to use when building a classifier that would be designed to predict whether or not a patient has heart disease. However, due to our limited knowledge at this time, we are not able to use the \"thal\" variable \n",
    "(indicating the status of the patient's thalassemia (a blood disorder)) since it is a categorical variable and can't be used in a KNN classifier. As such we will be using the following 3 *numerical* variables to build our KNN classifier:\n",
    "* Maximum heart rate achieved by the patient (bpm)\n",
    "* Age of the patient\n",
    "* Exercise-induced ST depression (measured by ECG)\n",
    "  \n",
    "As found in a 16-year study of initially healthy men, \"maximal exercise-induced heart rate \\[is] associated with cardiovascular mortality\" (Sandvik et al.). Age has also been found to impact a patient's risk of developing heart disease, as \"\\[h]eart disease is much more common in older age\" (Nutrition Health Review 14). Lastly, ST depression can \"indicate health conditions\" related to the patient's heart health (Rowden & Goodwin). Therefore, we chose these variables as they were found to have an association with our response variable (diagnosis) both in previous research as well as our preliminary data analysis.\n",
    "\n",
    "The question we are trying to answer with this predictive data analysis below is, **given the stated variables, can we predict if a patient has heart disease?**\n",
    "\n",
    "To answer this question, we will be using the Cleveland Heart Disease data set provided by Detrano et al., which contains biological information of potential heart disease patients at the Cleveland Clinic Foundation. In the entire data set, 139 of the patients were diagnosed with heart disease while 164 were not.\n",
    "\n",
    "## Methods\n",
    "First, we load the tidyverse package to be able to perform data analysis and visualization, along with the tidymodels package which is used for classification. We also load the gridExtra package which is used for our visualisations. This data analysis requires that the tidyverse, tidymodels, **ggmosaic** and gridExtra R packages be downloaded.\n",
    "\n",
    "Later on in the methods we use **ggmosaic**, an extension library to ggplot, to create a visualisation of our data analysis. Thus we first had to install ggmosaic locally in order to work with it. To do that, we ran the following code **in the console** (right click on cell -> new console for notebook):\n",
    "\n",
    "1. Run `install.packages(\"remotes\", lib = \"packages\")` to install the remotes package, which is used to download packages from GitHub, into a local folder called \"packages\" (need to create that folder first otherwise it won't write)\n",
    "2. Run `remotes::install_github(\"haleyjeppson/ggmosaic\", lib = \"packages\")` to install ggmosaic from its location on GitHub using remotes into the local packages folder\n",
    "3. Use `library(ggmosaic, lib.loc = \"packages\")` to load ggmosaic from the local folder into the R notebook\n",
    "\n",
    "**Note:** ggmosaic requires that htmltools be version 0.5.7 or later. If an older version is installed, run `install.packages(\"htmltools\")` in the console to update it (NOT locally) to the latest version.\n",
    "\n",
    "The kernel will need to be restarted after installing all of these packages in order for the code below to load ggmosaic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720e72d6-ff07-4abe-a49e-9783a3383568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.3.2”\n",
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.3     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.5.0     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.2     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 1.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.5     \u001b[32m✔\u001b[39m \u001b[34mrsample     \u001b[39m 1.2.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials       \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mtune        \u001b[39m 1.1.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer       \u001b[39m 1.0.5     \u001b[32m✔\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.3\n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.0.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip     \u001b[39m 1.1.1     \u001b[32m✔\u001b[39m \u001b[34myardstick   \u001b[39m 1.2.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.8     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m•\u001b[39m Use suppressPackageStartupMessages() to eliminate package startup messages\n",
      "\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: package or namespace load failed for ‘ggmosaic’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace ‘htmltools’ 0.5.6 is already loaded, but >= 0.5.7 is required\n",
     "output_type": "error",
     "traceback": [
      "Error: package or namespace load failed for ‘ggmosaic’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace ‘htmltools’ 0.5.6 is already loaded, but >= 0.5.7 is required\nTraceback:\n",
      "1. library(ggmosaic, lib.loc = \"packages\")",
      "2. tryCatch({\n .     attr(package, \"LibPath\") <- which.lib.loc\n .     ns <- loadNamespace(package, lib.loc)\n .     env <- attachNamespace(ns, pos = pos, deps, exclude, include.only)\n . }, error = function(e) {\n .     P <- if (!is.null(cc <- conditionCall(e))) \n .         paste(\" in\", deparse(cc)[1L])\n .     else \"\"\n .     msg <- gettextf(\"package or namespace load failed for %s%s:\\n %s\", \n .         sQuote(package), P, conditionMessage(e))\n .     if (logical.return && !quietly) \n .         message(paste(\"Error:\", msg), domain = NA)\n .     else stop(msg, call. = FALSE, domain = NA)\n . })",
      "3. tryCatchList(expr, classes, parentenv, handlers)",
      "4. tryCatchOne(expr, names, parentenv, handlers[[1L]])",
      "5. value[[3L]](cond)",
      "6. stop(msg, call. = FALSE, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(gridExtra)\n",
    "\n",
    "library(ggmosaic, lib.loc = \"packages\") # load ggmosaic from the local folder after all of the above steps have been completed\n",
    "set.seed(54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb50b9-3274-4dbe-a100-54939c18781a",
   "metadata": {},
   "source": [
    "Next we read in the data from the file available on the web, first by unzipping the file and then accessing only the data set we want. Although the dataset was provided as a .data file from the original source, the file contains comma-separated values, so we use read_csv. Since the data has no column names, we will rename them all using information available at [https://archive.ics.uci.edu/dataset/45/heart+disease]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd37613-6062-4909-bf86-22717133861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url <- \"https://archive.ics.uci.edu/static/public/45/heart+disease.zip\"\n",
    "heart_temp <- tempfile()\n",
    "download.file(url, heart_temp)\n",
    "data <- read_csv(unzip(heart_temp, \"processed.cleveland.data\"), col_names = FALSE, show_col_types = FALSE) |>\n",
    "    rename(age = X1, sex = X2, chest_pain = X3, resting_blood_pressure = X4,\n",
    "           cholesterol = X5, fasting_blood_sugar = X6,\n",
    "           resting_ecg = X7, max_heart_rate = X8, exercise_angina = X9,\n",
    "           st_depression = X10, slope = X11, major_vessels = X12, thal = X13,\n",
    "           diagnosis = X14)\n",
    "unlink(heart_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089f7ba-8a5f-4cc7-b564-f6016c86e63a",
   "metadata": {},
   "source": [
    "Now since we don't want to use every variable in the data set, we will select only those that we selected during preliminary data analysis: age. maximum heart rate achieved, ST depression, and diagnosis. Also, the authors of the data have indicated that all values 1-4 of the diagnosis variable mean a positive diagnosis, and only 0 indicates a negative diagnosis, so we will combine values 1-4 into 1 to simplify the data because our goal is only to determine the presence of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a037b-fc79-457a-b3aa-c05a6989e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected <- data |>\n",
    "    select(age, max_heart_rate, st_depression, diagnosis) |>\n",
    "    mutate(diagnosis = as_factor(diagnosis)) |>\n",
    "    mutate(diagnosis = fct_recode(diagnosis, \"1\" = \"2\", \"1\" = \"3\", \"1\" = \"4\"))\n",
    "\n",
    "cat(\"Table 1: Preview of the heart disease data set\")\n",
    "head(data_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5cac4-fc35-4c46-b281-9acf21cf1a02",
   "metadata": {},
   "source": [
    "The table above gives a sense of the data we are working with. Now we can split the data set into a training set, which will be used to build our classifier, and a testing set, which will be used to evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba7866-6f6d-4031-9e19-a57e697c75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split <- data_selected |>\n",
    "    mutate(diagnosis = fct_recode(diagnosis,\n",
    "                                \"Angiographic disease\" = \"1\",\n",
    "                                \"Healthy\" = \"0\")) |>\n",
    "    initial_split(prop = 0.75, strata = diagnosis)\n",
    "\n",
    "training_set <- training(data_split)\n",
    "testing_set <- testing(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f1e49-7160-4a5f-b77b-d1e600fb70ab",
   "metadata": {},
   "source": [
    "Before we begin the data analysis, a summary of the training data is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b7243-a959-4600-b07f-5a9df0336eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_observations <- training_set |>\n",
    "    group_by(diagnosis) |>\n",
    "    summarize(count = n())\n",
    "\n",
    "cat(\"Table 2: Number of patients in the data set that are healthy or have heart disease\")\n",
    "number_of_observations\n",
    "\n",
    "mean_values <- training_set |>\n",
    "    group_by(diagnosis) |>\n",
    "    summarize(mean_age = mean(age), mean_max_maximum_heart_rate = mean(max_heart_rate), mean_st_depression = mean(st_depression))\n",
    "\n",
    "cat(\"\\n\\nTable 3: Mean values of predictor variables for healthy vs. diagnosed patients\")\n",
    "mean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0f5f1-14d1-4fde-ba19-7e3ee3a0c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_age <- training_set |>\n",
    "    ggplot(aes(x = age, fill = diagnosis)) +\n",
    "    geom_histogram(position = \"dodge\", binwidth = 2) +\n",
    "    labs(x = \"Age\", y = \"Count\", fill = \"Diagnosis\") +\n",
    "    ggtitle(\"Figure 1: Distribution of age for patients with\\nheart disease vs. healthy\") +\n",
    "    theme(text = element_text(size = 10)) +\n",
    "    theme_minimal() +\n",
    "    scale_fill_manual(values = c(\"darkorange\", \"steelblue\"))\n",
    "\n",
    "graph_heart_rate <- training_set |>\n",
    "    ggplot(aes(x = max_heart_rate, fill = diagnosis)) +\n",
    "    geom_histogram(position = \"dodge\", binwidth = 4) +\n",
    "    labs(x = \"Maximum Heart Rate Achieved\", y = \"Count\", fill = \"Diagnosis\") +\n",
    "    ggtitle(\"Figure 2: Distribution of maximum achieved heart rate\\nfor patients with heart disease vs. healthy\") +\n",
    "    theme(text = element_text(size = 10)) +\n",
    "    theme_minimal() +\n",
    "    scale_fill_manual(values = c(\"darkorange\", \"steelblue\"))\n",
    "\n",
    "options(repr.plot.width = 11, repr.plot.height = 4)\n",
    "grid.arrange(graph_age, graph_heart_rate, ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cbc29-facd-493a-8d8f-78c2a18e227e",
   "metadata": {},
   "source": [
    "Now we will use cross-validation with 5 folds to determine the best K value to use for our K-nearest-neighbours classifier. We chose a range of K values from 5 to 20, since the data set only has 303 observations, and the training set is even smaller. We also create a recipe for pre-processing the data which involves standardized the data by scaling it (so that all variables have a standard deviation of 1) and centering it (so that all variables have a mean of 0). This standardization makes sure that all variables are traeated equally when the K nearest neighbors are found using distance, as they will now all be on the same scale, no 1 variable will have a greater influence than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905913a4-f0be-4f2a-84f8-68016b5c070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_recipe <- recipe(diagnosis ~ ., data = training_set) |>\n",
    "    step_scale(all_predictors()) |>\n",
    "    step_center(all_predictors())\n",
    "\n",
    "knn_spec_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "k_vals <- tibble(neighbors = 5:20)\n",
    "\n",
    "vfold <- vfold_cv(training_set, v = 10, strata = diagnosis)\n",
    "\n",
    "knn_fit_tuning <- workflow() |>\n",
    "    add_recipe(knn_recipe) |>\n",
    "    add_model(knn_spec_tune) |>\n",
    "    tune_grid(resamples = vfold, grid = k_vals)\n",
    "\n",
    "tuning_metrics <- knn_fit_tuning |>\n",
    "    collect_metrics()\n",
    "\n",
    "tuning_accuracies <- filter(tuning_metrics, .metric == \"accuracy\")\n",
    "\n",
    "cross_val_plot <- tuning_accuracies |>\n",
    "    ggplot(aes(x = neighbors, y = mean)) +\n",
    "    geom_point() + \n",
    "    geom_line() +\n",
    "    labs(x = \"K value\", y = \"Mean accuracy\") +\n",
    "    ggtitle(\"Figure 3: Average accuracy of KNN classifier based on number of neighbors used\") +\n",
    "    theme(text = element_text(size = 15)) +\n",
    "    theme_minimal() +\n",
    "    scale_x_continuous(breaks = 5:30)\n",
    "\n",
    "options(repr.plot.height = 5, repr.plot.width = 10)\n",
    "cross_val_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be370495-2062-4b1c-864a-f7644dcdaebd",
   "metadata": {},
   "source": [
    "From the above graph, we choose K to be 10 for the rest of our classifier. We chose 10 instead of 9 even though they have the same accuracy, since around a K of 10 (i.e. K = 9 and K = 11) the accuracies are higher than around K = 9 (i.e. there is a sudden drop off in accuracy when K = 8), so choosing a K where the K +/- 1 also results in similar accuracies provides some room for error that may be caused by variations.\n",
    "\n",
    "Below we build the model and fit it to our data using our selected K value of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421e9ab-50f5-457c-a91c-cc294baf4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 10) |>\n",
    "    set_engine(\"kknn\") |>\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "knn_fit <- workflow() |>\n",
    "    add_recipe(knn_recipe) |>\n",
    "    add_model(knn_spec) |>\n",
    "    fit(data = training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a26ad-b908-49d2-a888-744f30d2f8c3",
   "metadata": {},
   "source": [
    "Now we will evaluate our classifier using the testing set from the original data. To do this we first calculate the accuracy using the `metrics` function. The accuracy of a classifier represents the fraction of predictions for which the classifier makes the correct prediction, in other words,\n",
    "$$\\text{accuracy} = \\text{number of correct predictions } / \\text{ total predictions}.$$\n",
    "\n",
    "Since we are particularly interested in classifying and identifying patients with heart disease, we set this as our \"important\"/\"positive\" class, allowing us to calculate precision and recall. Precision refers to how many of the positive predictions our classifier made were actually positive. Recall refers to how many of the positive cases in the testing set our classifier actually labelled as positive.\n",
    "\n",
    "Precision is calculated as # of true positives / all positive predictions (i.e. true positives + false positives).\n",
    "\n",
    "Recall is calculated as # of true positives / # of positive test data points (i.e. true positives + false negatives).\n",
    "\n",
    "Note that in order be able to set the \"positive\" class when calculting precision and recall using the `event_level` parameter, we first need to determine the order in which the factor levels \"Healthy\" and \"Angiographic disease\" occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d676df-a939-407f-bd4c-1c404cec353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions <- predict(knn_fit, testing_set) |>\n",
    "    bind_cols(testing_set)\n",
    "\n",
    "accuracy <- predictions |>\n",
    "    metrics(truth = diagnosis, estimate = .pred_class) |>\n",
    "    filter(.metric == \"accuracy\")\n",
    "\n",
    "cat(\"Order of diagnoses in the data set:\")\n",
    "predictions |> pull(diagnosis) |> levels()\n",
    "\n",
    "precision <- predictions |>\n",
    "  precision(truth = diagnosis, estimate = .pred_class, event_level = \"second\")\n",
    "\n",
    "recall <- predictions |>\n",
    "  recall(truth = diagnosis, estimate = .pred_class, event_level = \"second\")\n",
    "\n",
    "# combine accuracy, precision and recall into 1 table\n",
    "metrics <- bind_rows(precision, recall, accuracy) |>\n",
    "    mutate(Metric = .metric, \"Value (%)\" = .estimate * 100) |>\n",
    "    select(Metric, \"Value (%)\")\n",
    "\n",
    "cat(\"\\n\\nTable 4: Metrics for KNN classifier built with K = 10\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d68f0-7703-4399-a062-7c7d7e06f44c",
   "metadata": {},
   "source": [
    "We also calculate the confusion matrix, which is a convenient way of analysing the types of predictions our classifier makes. This is because by labelling one class as \"positive\" (in this case, a diagnosis of heart disease), we can now count the number of:\n",
    "* True positives (correctly diagnosed with heart disease)\n",
    "* False positives (incorrectly diagnosed with heart disease)\n",
    "* True negatives (correctly diagnosed as healthy)\n",
    "* False negatives (incorrecly labelled as healthy)\n",
    "\n",
    "that our classifier makes. This is useful in evaluating our classifier and understanding what kinds of mistakes it is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc228ac-dd26-4261-9a9b-2a54e8b7bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat <- predictions |>\n",
    "    conf_mat(truth = diagnosis, estimate = .pred_class)\n",
    "\n",
    "cat(\"Table 5: Confusion matrix for KNN classifier with K = 10\")\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b45425-7fd3-4d0d-ab14-a48ee578d2cf",
   "metadata": {},
   "source": [
    "This is a matrix, so to better work with the data we transform it to a table and label it to be able to see exactly how our classifier is diagnosing individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e71450-0dfa-43aa-aec7-ccde91914a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_tbl <- tidy(conf_mat)\n",
    "\n",
    "conf_mat_tbl[1, \"name\"] <- \"True Negatives\"\n",
    "conf_mat_tbl[2, \"name\"] <- \"False Positives\"\n",
    "conf_mat_tbl[3, \"name\"] <- \"False Negatives\"\n",
    "conf_mat_tbl[4, \"name\"] <- \"True  Positives\"\n",
    "\n",
    "cat(\"Table 6: Labelled data from confusion matrix\")\n",
    "conf_mat_tbl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a588ad57-2c1f-4ada-94c0-7fec5e2c4bd2",
   "metadata": {},
   "source": [
    "In order to visualise this data analysis, we will create a mosaic plot of the confusion matrix which will visually show the proportions of each label. To make a mosaic plot, we use the ggmosaic package installed at the beginning of this section. The way the `geom_mosaic` function works is using proportions in the data instead of numerical values (which is what we currently have, as seen in Table 6). So in order to have the table store these numbers as proportions of observations, we pull the number `n` from each row of Table 6 and use it create `n` rows of a corresponding observation in a new table, called `mosaic_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3f0fa-c425-4525-9d46-95579e76e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_data <- uncount(tibble(Truth = \"Healthy (-)\", Predicted = \"Healthy (-)\"), pull(conf_mat_tbl[1, 2])) |> # true negatives\n",
    "    bind_rows(uncount(tibble(Truth = \"Healthy (-)\", Predicted = \"Diseased (+)\"), pull(conf_mat_tbl[2, 2]))) |> # false positives\n",
    "    bind_rows(uncount(tibble(Truth = \"Diseased (+)\", Predicted = \"Healthy (-)\"), pull(conf_mat_tbl[3, 2]))) |> # false negatives\n",
    "    bind_rows(uncount(tibble(Truth = \"Diseased (+)\", Predicted = \"Diseased (+)\"), pull(conf_mat_tbl[4, 2]))) # true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49611e3b-0c39-463b-8281-1c3dd7084d2a",
   "metadata": {},
   "source": [
    "Now we can use this data to build a mosaic plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22af49-e5be-4b16-a093-7a864c8a6ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_mosaic_plot <- ggplot(data = mosaic_data) +\n",
    "    geom_mosaic(aes(x = product(Truth, Predicted), fill = Predicted)) + \n",
    "    ggtitle(\"Figure 4: Mosaic plot of confusion matrix\") +\n",
    "    theme(text = element_text(size = 18)) +\n",
    "    theme_minimal() +\n",
    "    geom_mosaic_text(aes(x = product(Truth, Predicted), label = after_stat(.wt)), as.label = TRUE) +\n",
    "    scale_fill_brewer(palette = \"Paired\")\n",
    "\n",
    "confusion_matrix_mosaic_plot <- confusion_matrix_mosaic_plot +\n",
    "    geom_text(data = ggplot_build(confusion_matrix_mosaic_plot)$data[[1]] |>\n",
    "                      mutate(l = c(\"True Positive\", \"False Positive\", \"False Negative\", \"True Negative\"),\n",
    "                            col = c(\"b\", \"b\", \"w\", \"w\")), \n",
    "             aes(x = (xmin + xmax)/2, y = (ymin + ymax)/2 + 0.1, label = l, color = col)) +\n",
    "    scale_color_manual(values = c(\"black\", \"white\")) +\n",
    "    guides(color = \"none\")\n",
    "\n",
    "options(repr.plot.width = 7, repr.plot.height = 5)\n",
    "confusion_matrix_mosaic_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e9230-2d2a-419a-aaf0-5baf642e8b47",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "* Summarize what you found\n",
    "\n",
    "After evaluating our k-means classification model on our testing dataset, our performance metrics showcase an approximately `63.2%` accuracy, `60.6%` precision and `57.1%` recall. According to Table 4, our accuracy metric indicates that we can correctly diagnose whether a patient has heart disease or not at a rate of 63.2%. Our precision metric indicates that our model could correctly predict the presence of heart disease at a rate of 60.6% when it is assigning positive classifications. As for our recall metric, it is indicated that our model only has a 57.1% effectiveness when identifying patients with heart disease amongst all patients who truly do have heart disease. \n",
    "\n",
    "* Discuss whether this is what you expected to find\n",
    "\n",
    "Our predictive question was whether or not the aforementioned variables could indicate whether or not a patient had heart disease. In the context of diagnosing heart disease, high accuracy is important as committing false negatives would mean mislabelling patients as healthy when they truly do have heart disease, likely risking their lives. Thus, we were expecting a high accuracy for our classifier. \n",
    "\n",
    "Given that we had chosen relevant predictor variables (age, heart rate, depression) -- whilst avoiding high-dimensionality -- and fine-tuned our model parameters, we were expecting an accuracy of around 70-80% at least. The gap between our predicted and actual performance metrics may be attributed to the small sample size of our data. Since we have to split our data into a training and testing set, our small sample size has a high risk of bias, which may not be representative of the underlying population of heart disease patients, therefore leading to inaccurate predictions. \n",
    "\n",
    "Overall, we expected a balance between accuracy, precision and recall. For instance, a high recall but low precision would mean classifying many patients as having heart disease despite being healthy, this would incur unnecessary medical costs and treatments. Although we achieved a moderate balance between the three metrics, the performance of the classifier is not viable enough for it to be used in a real-world medical setting, as there is too much room for error. \n",
    "\n",
    "* Discuss what impact such findings could have\n",
    "\n",
    "Our model’s accuracy value is too low to be directly used as a diagnosis tool, but our findings will still have an impact on future research in this area. Given our accuracy value of 63.2%, using our predictor variables will still yield a prediction better than a random guess (which would be an accuracy value of 50%). This indicates that our choices of predictor variables are relevant and appropriate for predicting heart disease and that with further research, we'll be able to increase the accuracy. In summary, our findings will aid future research into prediction models for heart disease presence by affirming the variables of age, maximum heart rate achieved by the patient, and exercise-induced ST depression as relevant predictor variables.\n",
    "\n",
    "* Discuss what further questions this could lead to\n",
    "\n",
    "**What other relevant variables can/should be used in a classification model aiming to predict heart disease?**\n",
    "\n",
    "- Given our relatively low accuracy value, it may be beneficial to the accuracy value if we include new relevant variables. Exploring this question may allow for the creation of a better model.\n",
    "\n",
    "**How much does each individual variable available in the heart disease dataset contribute to the risk of heart disease?**\n",
    "\n",
    "- Exploring this question will allow for us to know how relevant each variable is to heart disease prediction, and whether they should be included as a predictor variable.\n",
    "\n",
    "**What other diseases can the k-nearest neighbour classification model be applied to?**\n",
    "\n",
    "- It may be possible to use the model (with different predictor variables) for other diseases, so this will allow for further research into prediction models for other diseases.\n",
    "\n",
    "\n",
    "# Work Cited\n",
    "Detrano, Robert, et al. “Heart Disease.” UCI Machine Learning Repository, 1988, archive.ics.uci.edu/dataset/45/heart+disease. Accessed 10 Apr. 2024. <br>\n",
    "\n",
    "\"Risk factors in middle age linked to more heart disease.\" Nutrition Health Review, no. 107, fall 2011, p. 14. Gale OneFile: Health and Medicine, link.gale.com/apps/doc/A302770509/HRCA?u=ubcolumbia&sid=summon&xid=661cd869. Accessed 10 Apr. 2024. <br>\n",
    "\n",
    "Rowden, Adam, and Meredith Goodwin. “St Depression on ECG: What It Means and Causes.” Medical News Today, MediLexicon International, 9 July 2022, www.medicalnewstoday.com/articles/st-depression-on-ecg. Accessed 10 Apr. 2024. <br>\n",
    "\n",
    "Sandvik, L et al. “Heart rate increase and maximal heart rate during exercise as predictors of cardiovascular mortality: a 16-year follow-up study of 1960 healthy men.” Coronary artery disease vol. 6,8 (1995): 667-79. doi:10.1097/00019501-199508000-00012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37209-cfb6-4cbb-b8c0-75548dc8b5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
